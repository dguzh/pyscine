{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Identify and filter locations\n",
    "Look for location candidates in the extracted text using NLTK and Stanford NER, then filter location candidates based on rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tag import StanfordNERTagger\n",
    "# nltk customization\n",
    "nltk.data.path.append('data_NLTK')\n",
    "StanfordBaseDir = ''\n",
    "os.environ['CLASSPATH'] = StanfordBaseDir + 'data_NER\\\\stanford-ner-2020-11-17\\\\'\n",
    "os.environ['STANFORD_MODELS'] = StanfordBaseDir + 'data_NER\\\\stanford-ner-2020-11-17\\\\classifiers'\n",
    "os.environ['JAVAHOME'] = 'C:\\\\Program Files\\\\Java\\\\jre1.8.0_351\\\\bin\\\\java.exe'\n",
    "# nltk initialization of Stanford NER tagger\n",
    "tagger = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n",
    "\n",
    "# own files\n",
    "from pysci import docutils as du\n",
    "from pysci import geoparse as gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_pickle = 'science_articles.pkl'\n",
    "# if needed, re-serialize in addition to any CSV export\n",
    "path_to_repickle = 'science_articles_geoparsed.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the serialized ScienceDocs\n",
    "science_docs = du.load_data(path_to_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from the ScienceDoc instances\n",
    "for scidoc in science_docs:\n",
    "    print(\"\\n### Processing article %s...\" %scidoc.file_name)\n",
    "\n",
    "    ### PARSE TITLE FROM XML ###\n",
    "    if scidoc.has_xml:\n",
    "        if scidoc.title:\n",
    "            scidoc.title_locations = []\n",
    "            title_string = scidoc.title\n",
    "            #print(\"Title: %s\" %title_string)\n",
    "            # process title\n",
    "            title_clean = gp.multireplace(title_string)\n",
    "            sent_tok = nltk.word_tokenize(title_clean)\n",
    "            sent_pos = nltk.pos_tag(sent_tok)\n",
    "            sent_ner = tagger.tag(sent_tok)\n",
    "            # customizable extract method\n",
    "            extracted_chunks = gp.extract_chunks_from_sentence(\n",
    "                sent_ner, include_cardinal=True, include_other_spatial=True, include_types=True)\n",
    "            if extracted_chunks:\n",
    "                print(\"### Found %s location candidates in title:\" %len(extracted_chunks))\n",
    "                for loc_chunk in extracted_chunks:\n",
    "                    loc_chunk_str = gp.tuple_list_to_string(loc_chunk)\n",
    "                    print(\"\\t%s\" %loc_chunk_str)\n",
    "                extracted_chunks_pos = gp.filter_chunk_candidates(sent_tok, extracted_chunks, verbose=True)\n",
    "                print(\"### Kept %s location chunks in title:\" %len(extracted_chunks_pos))\n",
    "                for loc_chunk_keep in extracted_chunks_pos:\n",
    "                    loc_chunk_keep_str = gp.tuple_list_to_string(loc_chunk_keep)\n",
    "                    # keep just the final filtered locations - empty list means we had none\n",
    "                    scidoc.title_locations.append(loc_chunk_keep_str)\n",
    "                    print(\"\\t%s\" %loc_chunk_keep_str)\n",
    "        else:\n",
    "            #print(\"No title for this article.\")\n",
    "            scidoc.title_locations = gp.NO_TITLE_STRING  \n",
    "    else:\n",
    "        #print(\"No xml file for this article.\")\n",
    "        scidoc.title_locations = gp.NO_XML_STRING\n",
    "\n",
    "    ### Process article contents\n",
    "    content_locations = []\n",
    "    content_locations_filtered = []\n",
    "    location_sentences = []\n",
    "    for par in re.split('[\\n]{2,}', scidoc.relevant_text):\n",
    "        par_clean = gp.multireplace(par)\n",
    "        #print(\"Clean paragraph: %s\" % (par_clean))\n",
    "        sentences = nltk.sent_tokenize(par_clean)\n",
    "        for sent in sentences:\n",
    "            sent_added = False\n",
    "            sent_tok = nltk.word_tokenize(sent)\n",
    "            sent_pos = nltk.pos_tag(sent_tok)\n",
    "            sent_ner = tagger.tag(sent_tok)\n",
    "            # customizable extract method\n",
    "            extracted_chunks = gp.extract_chunks_from_sentence(\n",
    "                sent_ner, include_cardinal=True, include_other_spatial=True, include_types=True)\n",
    "            if extracted_chunks:\n",
    "                #print(\"NER tagged sentence:\\n %s\" %sent_ner)\n",
    "                print(\"### Found %s location candidates in sentence:\" %len(extracted_chunks))\n",
    "                for loc_chunk in extracted_chunks:\n",
    "                    loc_chunk_str = gp.tuple_list_to_string(loc_chunk)\n",
    "                    content_locations.append(loc_chunk_str)\n",
    "                    print(\"\\t%s\" %loc_chunk_str)\n",
    "                extracted_chunks_pos = gp.filter_chunk_candidates(sent_tok, extracted_chunks, verbose=True)\n",
    "                print(\"### Kept %s location chunks:\" %len(extracted_chunks_pos))\n",
    "                for loc_chunk_keep in extracted_chunks_pos:\n",
    "                    loc_chunk_keep_str = gp.tuple_list_to_string(loc_chunk_keep)\n",
    "                    content_locations_filtered.append(loc_chunk_keep_str)\n",
    "                    print(\"\\t%s\" %loc_chunk_keep_str)\n",
    "                    if not sent_added:\n",
    "                        sent_no_breaks = sent.replace('\\n', ' ')\n",
    "                        location_sentences.append(sent_no_breaks)\n",
    "                        sent_added = True\n",
    "            \n",
    "    scidoc.content_locations = content_locations\n",
    "    scidoc.content_locations_filtered = content_locations_filtered\n",
    "    scidoc.location_sentences = location_sentences\n",
    "    \n",
    "print(\"\\n### Done.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally repickle article data with locations etc\n",
    "du.pickle_data(science_docs, path_to_repickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare per-article results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten each list entry to a string instead of a list\n",
    "filenames_for_df = []\n",
    "use_xml_for_df = []\n",
    "methods_for_df = []\n",
    "titles_for_df = []\n",
    "title_locations_for_df = []\n",
    "content_locations_for_df = []\n",
    "content_locations_filtered_for_df = []\n",
    "location_sentences_for_df = []\n",
    "for doc in science_docs:\n",
    "    filenames_for_df.append(doc.file_name)\n",
    "    use_xml_for_df.append(doc.use_xml)\n",
    "    try:\n",
    "        titles_for_df.append(doc.title)\n",
    "    except AttributeError:\n",
    "        titles_for_df.append(gp.NO_TITLE_STRING)\n",
    "    if doc.title_locations == gp.NO_XML_STRING or doc.title_locations == gp.NO_TITLE_STRING:\n",
    "        title_locations_for_df.append(doc.title_locations)\n",
    "    else:\n",
    "        title_locations_for_df.append('; '.join([x for x in doc.title_locations]))\n",
    "    try:\n",
    "        methods_for_df.append(doc.methods_sections)\n",
    "    except AttributeError:\n",
    "        methods_for_df.append('')\n",
    "    if not doc.content_locations:\n",
    "        content_locations_for_df.append('')\n",
    "    elif doc.content_locations == gp.NO_METHODS_STRING:\n",
    "        content_locations_for_df.append(doc.content_locations)\n",
    "    else:\n",
    "        content_locations_for_df.append('; '.join([x for x in doc.content_locations]))\n",
    "    if not doc.content_locations_filtered:\n",
    "        content_locations_filtered_for_df.append('')\n",
    "    elif doc.content_locations_filtered == gp.NO_METHODS_STRING:\n",
    "        content_locations_filtered_for_df.append(doc.content_locations_filtered)\n",
    "    else:\n",
    "        content_locations_filtered_for_df.append('; '.join([x for x in doc.content_locations_filtered]))\n",
    "    if not doc.location_sentences:\n",
    "        location_sentences_for_df.append('')\n",
    "    else:\n",
    "        location_sentences_for_df.append(doc.location_sentences)\n",
    "\n",
    "df_geoparsed = pd.DataFrame({'filename_only':filenames_for_df, \n",
    "                            'use_xml':use_xml_for_df,\n",
    "                            'title':titles_for_df,\n",
    "                            'title_locations':title_locations_for_df,\n",
    "                            'methods_sections':methods_for_df,\n",
    "                            'content_locations':content_locations_for_df,\n",
    "                            'content_locations_filtered':content_locations_filtered_for_df,\n",
    "                            'location_sentences':location_sentences_for_df})\n",
    "\n",
    "# increase the column width display of pandas tables to view full cells\n",
    "#pd.options.display.max_colwidth = 500\n",
    "\n",
    "df_geoparsed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geoparsed.to_csv(os.path.join('results', 'articles_geoparsed.tsv'), sep='\\t', index=False, quotechar='\"', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare per-location results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now each list item (each final content location) will be a separate row in the df\n",
    "filenames_flat = []\n",
    "content_locations_filtered_flat = []\n",
    "location_sentences_flat = []\n",
    "use_xml_flat = []\n",
    "for doc in science_docs:\n",
    "    if not doc.content_locations_filtered:\n",
    "        # store the 'no location' case!\n",
    "        content_locations_filtered_flat.append(gp.NO_LOCATIONS_STRING)\n",
    "        location_sentences_flat.append(gp.NO_LOCATIONS_STRING)\n",
    "        use_xml_flat.append(doc.use_xml)\n",
    "        filenames_flat.append(doc.file_name)\n",
    "        continue\n",
    "    elif doc.content_locations_filtered == gp.NO_METHODS_STRING:\n",
    "        content_locations_filtered_flat.append(gp.NO_METHODS_STRING)\n",
    "        location_sentences_flat.append(gp.NO_METHODS_STRING)\n",
    "        use_xml_flat.append(doc.use_xml)  # we store 'N/A' already\n",
    "        filenames_flat.append(doc.file_name)\n",
    "    else:\n",
    "        for location in doc.content_locations_filtered:\n",
    "            content_locations_filtered_flat.append(location)\n",
    "            use_xml_flat.append(doc.use_xml)\n",
    "            filenames_flat.append(doc.file_name)\n",
    "            found_sentence = False\n",
    "            for sentence in doc.location_sentences:\n",
    "                if location in sentence:\n",
    "                    found_sentence = True\n",
    "                    location_sentences_flat.append(sentence)\n",
    "                    break\n",
    "            if not found_sentence:\n",
    "                location_sentences_flat.append('no exact sentence match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = pd.DataFrame({'filename':filenames_flat,\n",
    "                        'content_locations':content_locations_filtered_flat,\n",
    "                        'use_xml':use_xml_flat,\n",
    "                        'location_sentences':location_sentences_flat})\n",
    "\n",
    "df_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat.to_csv(os.path.join('results', 'locations.tsv'), sep='\\t', index=False, quotechar='\"', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
